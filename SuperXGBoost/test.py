#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SuperXGBoost ä½¿ç”¨ç¤ºä¾‹ç¨‹åº
æ¼”ç¤ºSuperXGBooståº“çš„ä¸»è¦åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€è¶…å‚æ•°ä¼˜åŒ–å’Œè¯„ä¼°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer, load_wine, load_digits
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥SuperXGBoostï¼ˆå‡è®¾å·²ç»ä¿å­˜ä¸ºXgboost.pyæ–‡ä»¶ï¼‰
from Xgboost import SuperXGBoost

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


def demo_classification():
    """
    åˆ†ç±»ä»»åŠ¡æ¼”ç¤º - ä¿®å¤ç‰ˆæœ¬
    """
    print("=" * 80)
    print("SuperXGBoost åˆ†ç±»ä»»åŠ¡æ¼”ç¤º")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®é›†
    print("\n1. åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†...")
    data = load_breast_cancer()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target, name='target')

    print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{y.value_counts()}")

    # 2. åˆå§‹åŒ–SuperXGBoostæ¨¡å‹
    print("\n2. åˆå§‹åŒ–SuperXGBoostæ¨¡å‹...")
    model = SuperXGBoost(
        task_type='classification',
        objective='binary:logistic',
        gpu_acceleration=False,  # å¦‚æœæœ‰GPUå¯ä»¥è®¾ç½®ä¸ºTrue
        experiment_tracking=False,  # å¦‚æœè¦ä½¿ç”¨MLflowå¯ä»¥è®¾ç½®ä¸ºTrue
        auto_feature_engineering=True,
        auto_feature_selection=True,
        memory_optimization=False,
        verbose=1,
        random_state=42
    )

    # 3. æ•°æ®åˆ†æå’Œæ¦‚å†µ
    print("\n3. æ•°æ®åˆ†æ...")
    profile = model.profile_data(X, y, compute_correlations=True)
    model.print_data_profile(sections=['overview', 'missing_values', 'target'])

    # 4. ç»¼åˆæ•°æ®å‡†å¤‡
    print("\n4. æ•°æ®å‡†å¤‡ï¼ˆåŒ…æ‹¬é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€å¼‚å¸¸å€¼å¤„ç†ï¼‰...")
    X_train, X_test, y_train, y_test = model.prepare_data(
        X, y,
        test_size=0.2,
        preprocessing=True,
        feature_engineering=True,
        feature_selection=True,
        handle_outliers=True,
        handle_imbalance=True
    )

    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
    print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")

    # 5. æ¨¡å‹è®­ç»ƒ
    print("\n5. æ¨¡å‹è®­ç»ƒ...")
    eval_set = [(X_test, y_test)]
    model.fit(
        X_train, y_train,
        eval_set=eval_set,
        early_stopping_rounds=20
    )

    # 6. åŸºç¡€è¯„ä¼°
    print("\n6. æ¨¡å‹è¯„ä¼°...")
    eval_results = model.evaluate(
        X_test, y_test,
        metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],
        detailed=True,
        data_already_processed=True
    )

    # 7. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
    print("\n7. è¶…å‚æ•°ä¼˜åŒ–...")

    # å®šä¹‰å‚æ•°ç©ºé—´
    param_grid = {
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'n_estimators': [100, 200, 300],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0]
    }

    # ç½‘æ ¼æœç´¢
    try:
        best_params, best_score = model.grid_search(
            X_train, y_train,
            param_grid=param_grid,
            cv=3,  # å‡å°‘CVæŠ˜æ•°ä»¥åŠ é€Ÿæ¼”ç¤º
            scoring='roc_auc'
        )

        print(f"æœ€ä½³å‚æ•°: {best_params}")
        print(f"æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {best_score:.4f}")

        # 8. ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒå’Œè¯„ä¼°
        print("\n8. ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è¯„ä¼°...")
        final_results = model.evaluate(X_test, y_test, detailed=True)
    except Exception as e:
        print(f"è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
        final_results = eval_results

    # 9. äº¤å‰éªŒè¯
    print("\n9. äº¤å‰éªŒè¯...")
    try:
        cv_results = model.cross_validate(X_train, y_train, cv=5)
    except Exception as e:
        print(f"äº¤å‰éªŒè¯å¤±è´¥: {e}")

    # 10. å¯è§†åŒ–ã€ä¿®å¤ç‰ˆæœ¬ã€‘
    print("\n10. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

    # ç‰¹å¾é‡è¦æ€§å›¾
    try:
        fig_importance = model.plot_feature_importance(top_n=15)  # ã€ä¿®å¤ã€‘ç§»é™¤plot=Falseå‚æ•°
        print("âœ… ç‰¹å¾é‡è¦æ€§å›¾ç”ŸæˆæˆåŠŸ")
        plt.show()
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾å¤±è´¥: {e}")

    # å­¦ä¹ æ›²çº¿
    try:
        fig_learning = model.plot_learning_curve()  # ã€ä¿®å¤ã€‘ç§»é™¤plot=Falseå‚æ•°
        print("âœ… å­¦ä¹ æ›²çº¿ç”ŸæˆæˆåŠŸ")
        plt.show()
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶å­¦ä¹ æ›²çº¿å¤±è´¥: {e}")

    # æ··æ·†çŸ©é˜µ
    try:
        y_pred = model.predict(X_test)
        fig_cm = model.plot_confusion_matrix(y_test, y_pred)  # ã€ä¿®å¤ã€‘ç§»é™¤plot=Falseå‚æ•°
        print("âœ… æ··æ·†çŸ©é˜µç”ŸæˆæˆåŠŸ")
        plt.show()
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶æ··æ·†çŸ©é˜µå¤±è´¥: {e}")

    # ROCæ›²çº¿
    try:
        y_prob = model.predict_proba(X_test)
        # ã€ä¿®å¤ã€‘å¤„ç†æ¦‚ç‡æ•°ç»„çš„å½¢çŠ¶
        if len(y_prob.shape) > 1 and y_prob.shape[1] > 1:
            y_prob_pos = y_prob[:, 1]
        else:
            y_prob_pos = y_prob
        fig_roc = model.plot_roc_curve(y_test, y_prob_pos)  # ã€ä¿®å¤ã€‘ç§»é™¤plot=Falseå‚æ•°
        print("âœ… ROCæ›²çº¿ç”ŸæˆæˆåŠŸ")
        plt.show()
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ROCæ›²çº¿å¤±è´¥: {e}")

    return model, eval_results


def demo_regression():
    """
    å›å½’ä»»åŠ¡æ¼”ç¤º
    """
    print("=" * 80)
    print("SuperXGBoost å›å½’ä»»åŠ¡æ¼”ç¤º")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†çš„æ›¿ä»£æ•°æ®ï¼‰
    print("\n1. åˆ›å»ºå›å½’æ•°æ®é›†...")
    from sklearn.datasets import make_regression

    X, y = make_regression(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        noise=0.1,
        random_state=42
    )

    # è½¬æ¢ä¸ºDataFrame
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    X = pd.DataFrame(X, columns=feature_names)
    y = pd.Series(y, name='target')

    print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"ç›®æ ‡å˜é‡ç»Ÿè®¡:\n{y.describe()}")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("\n2. åˆå§‹åŒ–SuperXGBoostå›å½’æ¨¡å‹...")
    model = SuperXGBoost(
        task_type='regression',
        objective='reg:squarederror',
        verbose=1,
        random_state=42
    )

    # 3. æ•°æ®å‡†å¤‡
    print("\n3. æ•°æ®å‡†å¤‡...")
    X_train, X_test, y_train, y_test = model.prepare_data(
        X, y,
        test_size=0.2,
        preprocessing=True,
        feature_engineering=True
    )

    # 4. æ¨¡å‹è®­ç»ƒ
    print("\n4. æ¨¡å‹è®­ç»ƒ...")
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

    # 5. æ¨¡å‹è¯„ä¼°
    print("\n5. æ¨¡å‹è¯„ä¼°...")
    eval_results = model.evaluate(
        X_test, y_test,
        metrics=['rmse', 'mae', 'r2', 'explained_variance'],
        detailed=True
    )

    # 6. å¯è§†åŒ–é¢„æµ‹ç»“æœ
    print("\n6. å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    y_pred = model.predict(X_test)

    plt.figure(figsize=(10, 8))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('çœŸå®å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title('å›å½’é¢„æµ‹ç»“æœ')
    plt.grid(True, alpha=0.3)
    plt.show()

    return model, eval_results


def demo_multiclass():
    """
    å¤šåˆ†ç±»ä»»åŠ¡æ¼”ç¤º - ä¿®å¤ç‰ˆæœ¬
    """
    print("=" * 80)
    print("SuperXGBoost å¤šåˆ†ç±»ä»»åŠ¡æ¼”ç¤º - ä¿®å¤ç‰ˆæœ¬")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®é›†
    print("\n1. åŠ è½½çº¢é…’æ•°æ®é›†...")
    data = load_wine()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target, name='target')

    print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{y.value_counts()}")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("\n2. åˆå§‹åŒ–SuperXGBoostå¤šåˆ†ç±»æ¨¡å‹...")
    model = SuperXGBoost(
        task_type='classification',
        objective='multi:softprob',
        verbose=1,
        random_state=42
    )

    # ã€é‡è¦ä¿®å¤ã€‘æ‰‹åŠ¨è®¾ç½®num_classå‚æ•°ï¼ˆè™½ç„¶fitæ–¹æ³•ä¼šè‡ªåŠ¨è®¾ç½®ï¼Œä½†è¿™é‡Œæ˜ç¡®è®¾ç½®ä»¥ç¡®ä¿ï¼‰
    num_classes = len(np.unique(y))
    model.set_params(num_class=num_classes)
    print(f"è®¾ç½®ç±»åˆ«æ•°é‡: {num_classes}")

    # 3. æ•°æ®å‡†å¤‡å’Œè®­ç»ƒ
    print("\n3. æ•°æ®å‡†å¤‡å’Œè®­ç»ƒ...")
    X_train, X_test, y_train, y_test = model.prepare_data(X, y, test_size=0.2)

    try:
        model.fit(X_train, y_train)
        print("âœ… å¤šåˆ†ç±»æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ å¤šåˆ†ç±»æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        return None, None

    # 4. è¯„ä¼°
    print("\n4. æ¨¡å‹è¯„ä¼°...")
    try:
        eval_results = model.evaluate(
            X_test, y_test,
            metrics=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
        )
        print("âœ… å¤šåˆ†ç±»æ¨¡å‹è¯„ä¼°æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ å¤šåˆ†ç±»æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        eval_results = {}

    return model, eval_results


def demo_parameter_tuning():
    """
    é«˜çº§å‚æ•°è°ƒä¼˜æ¼”ç¤º
    """
    print("=" * 80)
    print("SuperXGBoost é«˜çº§å‚æ•°è°ƒä¼˜æ¼”ç¤º")
    print("=" * 80)

    # æ‰“å°å‚æ•°æŒ‡å—
    print("\nå‚æ•°è°ƒä¼˜æŒ‡å—:")
    model = SuperXGBoost(task_type='classification')
    model.print_parameter_guide(param_type='basic')

    # ä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œå¿«é€Ÿæ¼”ç¤º
    data = load_breast_cancer()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 1. éšæœºæœç´¢æ¼”ç¤º
    print("\n1. éšæœºæœç´¢æ¼”ç¤º...")
    param_distributions = {
        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
        'max_depth': [3, 4, 5, 6, 7],
        'n_estimators': [100, 150, 200, 250, 300],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
    }

    try:
        best_params_random, best_score_random = model.random_search(
            X_train, y_train,
            param_distributions=param_distributions,
            n_iter=10,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥åŠ é€Ÿæ¼”ç¤º
            cv=3
        )

        print(f"éšæœºæœç´¢æœ€ä½³å‚æ•°: {best_params_random}")
        print(f"éšæœºæœç´¢æœ€ä½³å¾—åˆ†: {best_score_random:.4f}")
    except Exception as e:
        print(f"éšæœºæœç´¢å¤±è´¥: {e}")

    return model


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º - ä¿®å¤ç‰ˆæœ¬
    """
    print("SuperXGBoost åº“å®Œæ•´åŠŸèƒ½æ¼”ç¤º - ä¿®å¤ç‰ˆæœ¬")
    print("=" * 80)

    try:
        # 1. åˆ†ç±»ä»»åŠ¡æ¼”ç¤º
        print("\nå¼€å§‹åˆ†ç±»ä»»åŠ¡æ¼”ç¤º...")
        clf_model, clf_results = demo_classification()

        # 2. å›å½’ä»»åŠ¡æ¼”ç¤º
        print("\n\nå¼€å§‹å›å½’ä»»åŠ¡æ¼”ç¤º...")
        reg_model, reg_results = demo_regression()

        # 3. å¤šåˆ†ç±»ä»»åŠ¡æ¼”ç¤ºã€ä¿®å¤ç‰ˆæœ¬ã€‘
        print("\n\nå¼€å§‹å¤šåˆ†ç±»ä»»åŠ¡æ¼”ç¤º...")
        multi_model, multi_results = demo_multiclass()

        # 4. å‚æ•°è°ƒä¼˜æ¼”ç¤º
        print("\n\nå¼€å§‹å‚æ•°è°ƒä¼˜æ¼”ç¤º...")
        tuned_model = demo_parameter_tuning()

        print("\n" + "=" * 80)
        print("æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)

        # æ‰“å°ç»“æœæ‘˜è¦
        print("\nğŸ“Š ç»“æœæ‘˜è¦:")
        if clf_results and 'accuracy' in clf_results:
            print(f"âœ… äºŒåˆ†ç±»å‡†ç¡®ç‡: {clf_results['accuracy']:.4f}")
        if reg_results and 'r2' in reg_results:
            print(f"âœ… å›å½’RÂ²åˆ†æ•°: {reg_results['r2']:.4f}")
        if multi_results and 'accuracy' in multi_results:
            print(f"âœ… å¤šåˆ†ç±»å‡†ç¡®ç‡: {multi_results['accuracy']:.4f}")

    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()